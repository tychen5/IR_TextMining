{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R06725035 陳廷易\n",
    "* feature selection: chi-square, log likelihood ratios, expected mutual information\n",
    "* naive bayes classification\n",
    "* voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os,sys\n",
    "# import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_words.txt') as f:\n",
    "    stop_words_list = f.read().splitlines() #stop_list1\n",
    "stop_list2 = pickle.load(open('data/stop_list2.pkl','rb'))\n",
    "ps = PorterStemmer() # Stemming\n",
    "stop_words = set(stopwords.words('english')) #Stopword\n",
    "short = ['.', ',', '\"', \"\\'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', \"'at\",\n",
    "         \"_\",\"`\",\"\\'\\'\",\"--\",\"``\",\".,\",\"//\",\":\",\"___\",'_the','-',\"'em\",\".com\",\n",
    "                   '\\'s','\\'m','\\'re','\\'ll','\\'d','n\\'t','shan\\'t',\"...\",\"\\'ve\",'u']\n",
    "stop_words_list.extend(short)\n",
    "stop_words_list.extend(stop_list2)\n",
    "stop_words.update(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    tokens = [i for i in word_tokenize(texts.lower()) if i not in stop_words]  # Tokenization.# Lowercasing\n",
    "    token_result = ''\n",
    "    token_result_ = ''\n",
    "    for i,token in enumerate(tokens): #list2str\n",
    "        token_result += ps.stem(token) + ' '\n",
    "    token_result = ''.join([i for i in token_result if not i.isdigit()])\n",
    "    token_result = [i for i in word_tokenize(token_result) if i not in stop_words]\n",
    "    for i,token in enumerate(token_result):\n",
    "        token_result_ += token + ' '\n",
    "    return token_result_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "* tf-idf\n",
    "* chi-square\n",
    "* likelihood\n",
    "* PMI\n",
    "* EMI\n",
    "\n",
    "=> build dictionary in 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_df = pd.read_csv('data/dictionary.txt',header=None,index_col=None,sep=' ')\n",
    "terms = dict_df[1].tolist() #all terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['11', '19', '29', '113', '115', '169', '278', '301', '316', '317', '321', '324', '325', '338', '341'], '2': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13', '14', '15', '16'], '3': ['813', '817', '818', '819', '820', '821', '822', '824', '825', '826', '828', '829', '830', '832', '833'], '4': ['635', '680', '683', '702', '704', '705', '706', '708', '709', '719', '720', '722', '723', '724', '726'], '5': ['646', '751', '781', '794', '798', '799', '801', '812', '815', '823', '831', '839', '840', '841', '842'], '6': ['995', '998', '999', '1003', '1005', '1006', '1007', '1009', '1011', '1012', '1013', '1014', '1015', '1016', '1019'], '7': ['700', '730', '731', '732', '733', '735', '740', '744', '752', '754', '755', '756', '757', '759', '760'], '8': ['262', '296', '304', '308', '337', '397', '401', '443', '445', '450', '466', '480', '513', '533', '534'], '9': ['130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '145'], '10': ['31', '44', '70', '83', '86', '92', '100', '102', '305', '309', '315', '320', '326', '327', '328'], '11': ['240', '241', '243', '244', '245', '248', '250', '254', '255', '256', '258', '260', '275', '279', '295'], '12': ['535', '542', '571', '573', '574', '575', '576', '578', '581', '582', '583', '584', '585', '586', '588'], '13': ['485', '520', '523', '526', '527', '529', '530', '531', '532', '536', '537', '538', '539', '540', '541']}\n"
     ]
    }
   ],
   "source": [
    "with open('data/training.txt','r') as f:\n",
    "    train_id = f.read().splitlines()\n",
    "train_dict = {}\n",
    "for trainid in train_id:\n",
    "    trainid = trainid.split(' ')\n",
    "    trainid = list(filter(None, trainid))\n",
    "    train_dict[trainid[0]] = trainid[1:]\n",
    "print(train_dict) #class:doc_id\n",
    "train_dict = pickle.load(open('data/train_dict.pkl','rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'data/IRTM/'\n",
    "train_dict_ = {}\n",
    "class_token = []\n",
    "class_token_dict = {}\n",
    "for c,d in train_dict.items():\n",
    "    for doc in d:\n",
    "        f = open('data/IRTM/'+doc+'.txt')\n",
    "        texts = f.read()\n",
    "        f.close()\n",
    "        tokens_all = preprocess(texts)\n",
    "        tokens_all = tokens_all.split(' ')\n",
    "        tokens_all = list(set(filter(None,tokens_all)))\n",
    "        class_token.append(tokens_all)\n",
    "    class_token_dict[c]=class_token\n",
    "    class_token=[]\n",
    "# len(class_token_dict['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             score\n",
      "term              \n",
      "abandon         39\n",
      "abc             51\n",
      "abcnews.com      3\n",
      "abdallah         2\n",
      "abdel            3\n",
      "abdomin          2\n",
      "abduct           2\n",
      "abdul           53\n",
      "abdul-karim     11\n",
      "abdullah        40\n",
      "abhorr           2\n",
      "abid            48\n",
      "abidjan         36\n",
      "abil            80\n",
      "abl            319\n",
      "ablaz            3\n",
      "aboard          93\n",
      "abobo            3\n",
      "abolish          4\n",
      "abort            7\n",
      "abou            29\n",
      "abound           4\n",
      "abraham         10\n",
      "abroad          52\n",
      "abrog            2\n",
      "abrupt           5\n",
      "abruptli         4\n",
      "absenc           6\n",
      "absent          14\n",
      "absente          7\n",
      "...            ...\n",
      "young          103\n",
      "younger         20\n",
      "youngest         6\n",
      "youngster        3\n",
      "youth           15\n",
      "youzhni          2\n",
      "yu-              2\n",
      "yug            139\n",
      "yugoslav       139\n",
      "yugoslavia     130\n",
      "yugoslavian     15\n",
      "yuri             3\n",
      "yuxi             2\n",
      "zacharea         2\n",
      "zan             47\n",
      "zarko            6\n",
      "zawahri          5\n",
      "zeal             3\n",
      "zedek            2\n",
      "zein             4\n",
      "zell             3\n",
      "zemin            7\n",
      "zero             6\n",
      "zinni           18\n",
      "zivko            2\n",
      "zivkov           2\n",
      "zogbi            3\n",
      "zone            28\n",
      "zoran           20\n",
      "zuric            2\n",
      "\n",
      "[8913 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "dict_df.drop(0,axis=1,inplace=True)\n",
    "dict_df.columns = ['term','score']\n",
    "dict_df.index = dict_df['term']\n",
    "dict_df.drop('term',axis=1,inplace=True)\n",
    "print(dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8913/8913 [03:55<00:00, 37.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 score     score_chi     score_emi\n",
      "term                                              \n",
      "abandon       1.659552  3.846051e+00  2.913791e-03\n",
      "abc           0.899990  1.555876e+00  1.580176e-03\n",
      "abcnews.com   0.204845  6.544514e-01  3.596454e-04\n",
      "abdallah      0.274067  5.290393e-01  4.811811e-04\n",
      "abdel         0.411467  7.944005e-01  7.224236e-04\n",
      "abdomin       0.274067  5.290393e-01  4.811811e-04\n",
      "abduct        0.160386  3.668540e-01  2.815866e-04\n",
      "abdul         0.464110  9.267195e-01  8.148566e-04\n",
      "abdul-karim   1.519594  2.937724e+00  2.668045e-03\n",
      "abdullah      5.383307  1.045027e+01  9.451857e-03\n",
      "abhorr        0.599975  4.723560e+00  1.053402e-03\n",
      "abid          0.378774  8.999763e-01  6.650296e-04\n",
      "abidjan      10.299234  5.488540e+01  1.808312e-02\n",
      "abil          0.976184  2.298259e+00  1.713953e-03\n",
      "abl           0.970499  1.925329e+00  1.703978e-03\n",
      "ablaz         0.293008  1.039403e+00  5.144410e-04\n",
      "aboard        6.593954  1.392154e+01  1.157749e-02\n",
      "abobo         0.812945  4.683628e+00  1.427329e-03\n",
      "abolish       0.577250  1.702640e+00  1.013504e-03\n",
      "abort         0.677678  2.420350e+00  1.189835e-03\n",
      "abou          0.269511  1.557905e+00  4.731823e-04\n",
      "abound        0.360321  7.431483e-01  6.326256e-04\n",
      "abraham       1.472125  6.706604e+00  2.584704e-03\n",
      "abroad        2.623727  7.390832e+00  4.606668e-03\n",
      "abrog         0.000000  7.117401e-08  2.183305e-09\n",
      "abrupt        0.249705  1.181549e+00  4.384079e-04\n",
      "abruptli      0.253273  1.241571e+00  4.446725e-04\n",
      "absenc        0.600643  2.259850e+00  1.054579e-03\n",
      "absent        0.361637  1.007732e+00  6.349385e-04\n",
      "absente       1.352649  5.783335e+00  2.374932e-03\n",
      "...                ...           ...           ...\n",
      "young         2.752543  8.000081e+00  4.832844e-03\n",
      "younger       1.514666  4.640895e+00  2.659406e-03\n",
      "youngest      0.521708  1.384703e+00  9.159885e-04\n",
      "youngster     0.270136  8.087003e-01  4.742826e-04\n",
      "youth         0.659291  1.574781e+00  1.157561e-03\n",
      "youzhni       0.491417  2.222285e+00  8.627988e-04\n",
      "yu-           0.418265  1.354611e+00  7.343599e-04\n",
      "yug           0.000000  7.117401e-08  2.183305e-09\n",
      "yugoslav     15.820595  4.948838e+01  2.777740e-02\n",
      "yugoslavia   21.768024  6.407368e+01  3.821976e-02\n",
      "yugoslavian   1.814555  5.200910e+00  3.185933e-03\n",
      "yuri          0.507739  2.049276e+00  8.914598e-04\n",
      "yuxi          0.418265  1.354611e+00  7.343599e-04\n",
      "zacharea      0.507705  2.485772e+00  8.913961e-04\n",
      "zan           0.540482  3.119111e+00  9.489453e-04\n",
      "zarko         1.262433  4.081127e+00  2.216529e-03\n",
      "zawahri       0.411467  7.944005e-01  7.224236e-04\n",
      "zeal          0.136912  2.642397e-01  2.403682e-04\n",
      "zedek         0.000000  7.117401e-08  2.183305e-09\n",
      "zein          0.549112  1.060325e+00  9.640973e-04\n",
      "zell          0.738692  3.336962e+00  1.296958e-03\n",
      "zemin         1.228378  5.883284e+00  2.156738e-03\n",
      "zero          0.355899  8.665818e-01  6.248644e-04\n",
      "zinni         2.502540  4.843446e+00  4.393877e-03\n",
      "zivko         0.208819  6.765889e-01  3.666214e-04\n",
      "zivkov        0.418265  1.354611e+00  7.343599e-04\n",
      "zogbi         0.738692  3.336962e+00  1.296958e-03\n",
      "zone          2.180161  7.458516e+00  3.827861e-03\n",
      "zoran         4.080964  1.310480e+01  7.165236e-03\n",
      "zuric         0.208819  6.765889e-01  3.666214e-04\n",
      "\n",
      "[8913 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_df['score'] = 0\n",
    "dict_df['score_chi'] = 0\n",
    "dict_df['score_emi'] = 0\n",
    "# c=1\n",
    "for term in tqdm(terms): #each term\n",
    "    scores = []\n",
    "    scores_chi = []\n",
    "    scores_emi = []\n",
    "    c=1\n",
    "    for _ in range(len(class_token_dict)): # each class\n",
    "        n11=e11=m11=0\n",
    "        n10=e10=m10=0\n",
    "        n01=e01=m01=0\n",
    "        n00=e00=m00=0\n",
    "        for k,v in class_token_dict.items():\n",
    "#             print(k,c)\n",
    "            if k == str(c): #ontopic\n",
    "                for r in v:\n",
    "                    if term in r:\n",
    "                        n11+=1\n",
    "                    else:\n",
    "                        n10+=1\n",
    "#                 c+=1\n",
    "            else: #off topic\n",
    "                for r in v:\n",
    "                    if term in r:\n",
    "                        n01+=1\n",
    "                    else:\n",
    "                        n00+=1\n",
    "#                 c+=1\n",
    "        c+=1\n",
    "        n11+=1e-8\n",
    "        n10+=1e-8\n",
    "        n01+=1e-8\n",
    "        n00+=1e-8\n",
    "        N = n11+n10+n01+n00\n",
    "        e11 = N * (n11+n01)/N * (n11+n10)/N #chi-squre\n",
    "        e10 = N * (n11+n10)/N * (n10+n00)/N\n",
    "        e01 = N * (n11+n01)/N * (n01+n00)/N\n",
    "        e00 = N * (n01+n00)/N * (n10+n00)/N\n",
    "        score_chi = ((n11-e11)**2)/e11 + ((n10-e10)**2)/e10 + ((n01-e01)**2)/e01 + ((n00-e00)**2)/e00\n",
    "        scores_chi.append(score_chi)\n",
    "        \n",
    "        n11 = n11 - 1e-8 + 1e-6\n",
    "        n10 = n10 - 1e-8 + 1e-6\n",
    "        n01 = n01 - 1e-8 + 1e-6\n",
    "        n00 = n00 - 1e-8 + 1e-6\n",
    "        N = n11+n10+n01+n00\n",
    "        m11 = (n11/N) * math.log(((n11/N)/((n11+n01)/N * (n11+n10)/N)),2) #EMI\n",
    "        m10 = n10/N * math.log((n10/N)/((n11+n10)/N * (n10+n00)/N),2)\n",
    "        m01 = n01/N * math.log((n01/N)/((n11+n01)/N * (n01+n00)/N),2)\n",
    "        m00 = n00/N * math.log((n00/N)/((n01+n00)/N * (n10+n00)/N),2)\n",
    "        score_emi = m11 + m10 + m01 + m00\n",
    "        scores_emi.append(score_emi)\n",
    "        \n",
    "#         print(n11,n10,n01,n00)\n",
    "        n11-=1e-6\n",
    "        m10-=1e-6\n",
    "        n01-=1e-6\n",
    "        n00-=1e-6\n",
    "        N = n11+n10+n01+n00\n",
    "        score = (((n11+n01)/N) ** n11) * ((1 - ((n11+n01)/N)) ** n10) * (((n11+n01)/N) ** n01) * ((1 - ((n11+n01)/N)) ** n00)\n",
    "        score /= ((n11/(n11+n10)) ** n11) * ((1 - (n11/(n11+n10))) ** n10) * ((n01/(n01+n00)) ** n01) * ((1 - (n01/(n01+n00))) ** n00)\n",
    "        score = -2 * math.log(score, 10) #LLR\n",
    "        scores.append(score)\n",
    "        \n",
    "        \n",
    "#         c+=1\n",
    "    dict_df.loc[term,'score'] = np.mean(scores)\n",
    "    dict_df.loc[term,'score_chi'] = np.mean(scores_chi)\n",
    "    dict_df.loc[term,'score_emi'] = np.mean(scores_emi)\n",
    "print(dict_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id         term  freq  sum\n",
      "0        1      abandon    39  0.0\n",
      "1        2          abc    51  0.0\n",
      "2        3  abcnews.com     3  0.0\n",
      "3        4     abdallah     2  0.0\n",
      "4        5        abdel     3  0.0\n",
      "5        6      abdomin     2  0.0\n",
      "6        7       abduct     2  0.0\n",
      "7        8        abdul    53  0.0\n",
      "8        9  abdul-karim    11  0.0\n",
      "9       10     abdullah    40  0.0\n",
      "10      11       abhorr     2  0.0\n",
      "11      12         abid    48  0.0\n",
      "12      13      abidjan    36  0.0\n",
      "13      14         abil    80  0.0\n",
      "14      15          abl   319  0.0\n",
      "15      16        ablaz     3  0.0\n",
      "16      17       aboard    93  0.0\n",
      "17      18        abobo     3  0.0\n",
      "18      19      abolish     4  0.0\n",
      "19      20        abort     7  0.0\n",
      "20      21         abou    29  0.0\n",
      "21      22       abound     4  0.0\n",
      "22      23      abraham    10  0.0\n",
      "23      24       abroad    52  0.0\n",
      "24      25        abrog     2  0.0\n",
      "25      26       abrupt     5  0.0\n",
      "26      27     abruptli     4  0.0\n",
      "27      28       absenc     6  0.0\n",
      "28      29       absent    14  0.0\n",
      "29      30      absente     7  0.0\n",
      "...    ...          ...   ...  ...\n",
      "8883  8884        young   103  0.0\n",
      "8884  8885      younger    20  0.0\n",
      "8885  8886     youngest     6  0.0\n",
      "8886  8887    youngster     3  0.0\n",
      "8887  8888        youth    15  0.0\n",
      "8888  8889      youzhni     2  0.0\n",
      "8889  8890          yu-     2  0.0\n",
      "8890  8891          yug   139  0.0\n",
      "8891  8892     yugoslav   139  0.0\n",
      "8892  8893   yugoslavia   130  0.0\n",
      "8893  8894  yugoslavian    15  0.0\n",
      "8894  8895         yuri     3  0.0\n",
      "8895  8896         yuxi     2  0.0\n",
      "8896  8897     zacharea     2  0.0\n",
      "8897  8898          zan    47  0.0\n",
      "8898  8899        zarko     6  0.0\n",
      "8899  8900      zawahri     5  0.0\n",
      "8900  8901         zeal     3  0.0\n",
      "8901  8902        zedek     2  0.0\n",
      "8902  8903         zein     4  0.0\n",
      "8903  8904         zell     3  0.0\n",
      "8904  8905        zemin     7  0.0\n",
      "8905  8906         zero     6  0.0\n",
      "8906  8907        zinni    18  0.0\n",
      "8907  8908        zivko     2  0.0\n",
      "8908  8909       zivkov     2  0.0\n",
      "8909  8910        zogbi     3  0.0\n",
      "8910  8911         zone    28  0.0\n",
      "8911  8912        zoran    20  0.0\n",
      "8912  8913        zuric     2  0.0\n",
      "\n",
      "[8913 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "dict_df2 = pd.read_csv('data/dictionary.txt',header=None,index_col=None,sep=' ')\n",
    "dict_df2.columns = ['id','term','freq']\n",
    "dict_df2['sum'] = 0.0\n",
    "print(dict_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id         term  avg_tfidf\n",
      "0        1      abandon   0.005374\n",
      "1        2          abc   0.015857\n",
      "2        3  abcnews.com   0.033696\n",
      "3        4     abdallah   0.006597\n",
      "4        5        abdel   0.003924\n",
      "5        6      abdomin   0.010949\n",
      "6        7       abduct   0.016260\n",
      "7        8        abdul   0.000851\n",
      "8        9  abdul-karim   0.005135\n",
      "9       10     abdullah   0.006808\n",
      "10      11       abhorr   0.008349\n",
      "11      12         abid   0.000591\n",
      "12      13      abidjan   0.017659\n",
      "13      14         abil   0.001223\n",
      "14      15          abl   0.000976\n",
      "15      16        ablaz   0.006857\n",
      "16      17       aboard   0.006911\n",
      "17      18        abobo   0.005802\n",
      "18      19      abolish   0.006744\n",
      "19      20        abort   0.012137\n",
      "20      21         abou   0.000133\n",
      "21      22       abound   0.007865\n",
      "22      23      abraham   0.007256\n",
      "23      24       abroad   0.006570\n",
      "24      25        abrog   0.003322\n",
      "25      26       abrupt   0.001282\n",
      "26      27     abruptli   0.026259\n",
      "27      28       absenc   0.013634\n",
      "28      29       absent   0.001582\n",
      "29      30      absente   0.005075\n",
      "...    ...          ...        ...\n",
      "8883  8884        young   0.004997\n",
      "8884  8885      younger   0.004907\n",
      "8885  8886     youngest   0.006362\n",
      "8886  8887    youngster   0.005772\n",
      "8887  8888        youth   0.005203\n",
      "8888  8889      youzhni   0.007629\n",
      "8889  8890          yu-   0.004758\n",
      "8890  8891          yug   0.000041\n",
      "8891  8892     yugoslav   0.004176\n",
      "8892  8893   yugoslavia   0.011578\n",
      "8893  8894  yugoslavian   0.011594\n",
      "8894  8895         yuri   0.018397\n",
      "8895  8896         yuxi   0.010819\n",
      "8896  8897     zacharea   0.017215\n",
      "8897  8898          zan   0.000147\n",
      "8898  8899        zarko   0.006044\n",
      "8899  8900      zawahri   0.002351\n",
      "8900  8901         zeal   0.000789\n",
      "8901  8902        zedek   0.007111\n",
      "8902  8903         zein   0.006555\n",
      "8903  8904         zell   0.003836\n",
      "8904  8905        zemin   0.006830\n",
      "8905  8906         zero   0.019958\n",
      "8906  8907        zinni   0.023997\n",
      "8907  8908        zivko   0.002301\n",
      "8908  8909       zivkov   0.006340\n",
      "8909  8910        zogbi   0.009333\n",
      "8910  8911         zone   0.005806\n",
      "8911  8912        zoran   0.005915\n",
      "8912  8913        zuric   0.005658\n",
      "\n",
      "[8913 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "tf_list = next(os.walk('data/tf-idf/'))[2]\n",
    "# df_list = [dict_df2]\n",
    "for tf in tf_list:\n",
    "#     print(tf)\n",
    "    df2 = pd.read_csv('data/tf-idf/'+tf,header=None,index_col=None,sep=' ',skiprows=[0])\n",
    "    df2.columns = ['id','tfidf']\n",
    "    df3 = pd.merge(dict_df2,df2,on='id',how='outer')\n",
    "    df3.fillna(0,inplace=True)\n",
    "    dict_df2['sum']+=df3['tfidf']\n",
    "dict_df2['avg_tfidf'] = dict_df2['sum']/dict_df2['freq']\n",
    "dict_df2 = dict_df2.drop(['freq','sum'],axis=1)\n",
    "print(dict_df2)\n",
    "#     break\n",
    "#     df_list.append(df2)\n",
    "# df3 = pd.concat(df_list).groupby(level=0).sum()\n",
    "# df3 = pd.concat([df3,df2]).groupby(level=0).sum()\n",
    "# df3 = pd.merge(dict_df2,df2,on='id',how='outer')\n",
    "# df3 = pd.merge(df3,df2,on='id',how='outer')?\n",
    "# df3[df3.id == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          score     score_chi     score_emi         term    id  avg_tfidf\n",
      "0      1.659552  3.846051e+00  2.913791e-03      abandon     1   0.005374\n",
      "1      0.899990  1.555876e+00  1.580176e-03          abc     2   0.015857\n",
      "2      0.204845  6.544514e-01  3.596454e-04  abcnews.com     3   0.033696\n",
      "3      0.274067  5.290393e-01  4.811811e-04     abdallah     4   0.006597\n",
      "4      0.411467  7.944005e-01  7.224236e-04        abdel     5   0.003924\n",
      "5      0.274067  5.290393e-01  4.811811e-04      abdomin     6   0.010949\n",
      "6      0.160386  3.668540e-01  2.815866e-04       abduct     7   0.016260\n",
      "7      0.464110  9.267195e-01  8.148566e-04        abdul     8   0.000851\n",
      "8      1.519594  2.937724e+00  2.668045e-03  abdul-karim     9   0.005135\n",
      "9      5.383307  1.045027e+01  9.451857e-03     abdullah    10   0.006808\n",
      "10     0.599975  4.723560e+00  1.053402e-03       abhorr    11   0.008349\n",
      "11     0.378774  8.999763e-01  6.650296e-04         abid    12   0.000591\n",
      "12    10.299234  5.488540e+01  1.808312e-02      abidjan    13   0.017659\n",
      "13     0.976184  2.298259e+00  1.713953e-03         abil    14   0.001223\n",
      "14     0.970499  1.925329e+00  1.703978e-03          abl    15   0.000976\n",
      "15     0.293008  1.039403e+00  5.144410e-04        ablaz    16   0.006857\n",
      "16     6.593954  1.392154e+01  1.157749e-02       aboard    17   0.006911\n",
      "17     0.812945  4.683628e+00  1.427329e-03        abobo    18   0.005802\n",
      "18     0.577250  1.702640e+00  1.013504e-03      abolish    19   0.006744\n",
      "19     0.677678  2.420350e+00  1.189835e-03        abort    20   0.012137\n",
      "20     0.269511  1.557905e+00  4.731823e-04         abou    21   0.000133\n",
      "21     0.360321  7.431483e-01  6.326256e-04       abound    22   0.007865\n",
      "22     1.472125  6.706604e+00  2.584704e-03      abraham    23   0.007256\n",
      "23     2.623727  7.390832e+00  4.606668e-03       abroad    24   0.006570\n",
      "24     0.000000  7.117401e-08  2.183305e-09        abrog    25   0.003322\n",
      "25     0.249705  1.181549e+00  4.384079e-04       abrupt    26   0.001282\n",
      "26     0.253273  1.241571e+00  4.446725e-04     abruptli    27   0.026259\n",
      "27     0.600643  2.259850e+00  1.054579e-03       absenc    28   0.013634\n",
      "28     0.361637  1.007732e+00  6.349385e-04       absent    29   0.001582\n",
      "29     1.352649  5.783335e+00  2.374932e-03      absente    30   0.005075\n",
      "...         ...           ...           ...          ...   ...        ...\n",
      "8883   2.752543  8.000081e+00  4.832844e-03        young  8884   0.004997\n",
      "8884   1.514666  4.640895e+00  2.659406e-03      younger  8885   0.004907\n",
      "8885   0.521708  1.384703e+00  9.159885e-04     youngest  8886   0.006362\n",
      "8886   0.270136  8.087003e-01  4.742826e-04    youngster  8887   0.005772\n",
      "8887   0.659291  1.574781e+00  1.157561e-03        youth  8888   0.005203\n",
      "8888   0.491417  2.222285e+00  8.627988e-04      youzhni  8889   0.007629\n",
      "8889   0.418265  1.354611e+00  7.343599e-04          yu-  8890   0.004758\n",
      "8890   0.000000  7.117401e-08  2.183305e-09          yug  8891   0.000041\n",
      "8891  15.820595  4.948838e+01  2.777740e-02     yugoslav  8892   0.004176\n",
      "8892  21.768024  6.407368e+01  3.821976e-02   yugoslavia  8893   0.011578\n",
      "8893   1.814555  5.200910e+00  3.185933e-03  yugoslavian  8894   0.011594\n",
      "8894   0.507739  2.049276e+00  8.914598e-04         yuri  8895   0.018397\n",
      "8895   0.418265  1.354611e+00  7.343599e-04         yuxi  8896   0.010819\n",
      "8896   0.507705  2.485772e+00  8.913961e-04     zacharea  8897   0.017215\n",
      "8897   0.540482  3.119111e+00  9.489453e-04          zan  8898   0.000147\n",
      "8898   1.262433  4.081127e+00  2.216529e-03        zarko  8899   0.006044\n",
      "8899   0.411467  7.944005e-01  7.224236e-04      zawahri  8900   0.002351\n",
      "8900   0.136912  2.642397e-01  2.403682e-04         zeal  8901   0.000789\n",
      "8901   0.000000  7.117401e-08  2.183305e-09        zedek  8902   0.007111\n",
      "8902   0.549112  1.060325e+00  9.640973e-04         zein  8903   0.006555\n",
      "8903   0.738692  3.336962e+00  1.296958e-03         zell  8904   0.003836\n",
      "8904   1.228378  5.883284e+00  2.156738e-03        zemin  8905   0.006830\n",
      "8905   0.355899  8.665818e-01  6.248644e-04         zero  8906   0.019958\n",
      "8906   2.502540  4.843446e+00  4.393877e-03        zinni  8907   0.023997\n",
      "8907   0.208819  6.765889e-01  3.666214e-04        zivko  8908   0.002301\n",
      "8908   0.418265  1.354611e+00  7.343599e-04       zivkov  8909   0.006340\n",
      "8909   0.738692  3.336962e+00  1.296958e-03        zogbi  8910   0.009333\n",
      "8910   2.180161  7.458516e+00  3.827861e-03         zone  8911   0.005806\n",
      "8911   4.080964  1.310480e+01  7.165236e-03        zoran  8912   0.005915\n",
      "8912   0.208819  6.765889e-01  3.666214e-04        zuric  8913   0.005658\n",
      "\n",
      "[8913 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2961: FutureWarning: 'term' is both an index level and a column label.\n",
      "Defaulting to column, but this will raise an ambiguity error in a future version\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "dict_df['term'] = dict_df.index\n",
    "dict_df3 = pd.merge(dict_df,dict_df2,on='term',how='outer')\n",
    "print(dict_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id         term  avg_tfidf     score_chi      score     score_emi\n",
      "0        1      abandon   0.005374  3.846051e+00   1.659552  2.913791e-03\n",
      "1        2          abc   0.015857  1.555876e+00   0.899990  1.580176e-03\n",
      "2        3  abcnews.com   0.033696  6.544514e-01   0.204845  3.596454e-04\n",
      "3        4     abdallah   0.006597  5.290393e-01   0.274067  4.811811e-04\n",
      "4        5        abdel   0.003924  7.944005e-01   0.411467  7.224236e-04\n",
      "5        6      abdomin   0.010949  5.290393e-01   0.274067  4.811811e-04\n",
      "6        7       abduct   0.016260  3.668540e-01   0.160386  2.815866e-04\n",
      "7        8        abdul   0.000851  9.267195e-01   0.464110  8.148566e-04\n",
      "8        9  abdul-karim   0.005135  2.937724e+00   1.519594  2.668045e-03\n",
      "9       10     abdullah   0.006808  1.045027e+01   5.383307  9.451857e-03\n",
      "10      11       abhorr   0.008349  4.723560e+00   0.599975  1.053402e-03\n",
      "11      12         abid   0.000591  8.999763e-01   0.378774  6.650296e-04\n",
      "12      13      abidjan   0.017659  5.488540e+01  10.299234  1.808312e-02\n",
      "13      14         abil   0.001223  2.298259e+00   0.976184  1.713953e-03\n",
      "14      15          abl   0.000976  1.925329e+00   0.970499  1.703978e-03\n",
      "15      16        ablaz   0.006857  1.039403e+00   0.293008  5.144410e-04\n",
      "16      17       aboard   0.006911  1.392154e+01   6.593954  1.157749e-02\n",
      "17      18        abobo   0.005802  4.683628e+00   0.812945  1.427329e-03\n",
      "18      19      abolish   0.006744  1.702640e+00   0.577250  1.013504e-03\n",
      "19      20        abort   0.012137  2.420350e+00   0.677678  1.189835e-03\n",
      "20      21         abou   0.000133  1.557905e+00   0.269511  4.731823e-04\n",
      "21      22       abound   0.007865  7.431483e-01   0.360321  6.326256e-04\n",
      "22      23      abraham   0.007256  6.706604e+00   1.472125  2.584704e-03\n",
      "23      24       abroad   0.006570  7.390832e+00   2.623727  4.606668e-03\n",
      "24      25        abrog   0.003322  7.117401e-08   0.000000  2.183305e-09\n",
      "25      26       abrupt   0.001282  1.181549e+00   0.249705  4.384079e-04\n",
      "26      27     abruptli   0.026259  1.241571e+00   0.253273  4.446725e-04\n",
      "27      28       absenc   0.013634  2.259850e+00   0.600643  1.054579e-03\n",
      "28      29       absent   0.001582  1.007732e+00   0.361637  6.349385e-04\n",
      "29      30      absente   0.005075  5.783335e+00   1.352649  2.374932e-03\n",
      "...    ...          ...        ...           ...        ...           ...\n",
      "8883  8884        young   0.004997  8.000081e+00   2.752543  4.832844e-03\n",
      "8884  8885      younger   0.004907  4.640895e+00   1.514666  2.659406e-03\n",
      "8885  8886     youngest   0.006362  1.384703e+00   0.521708  9.159885e-04\n",
      "8886  8887    youngster   0.005772  8.087003e-01   0.270136  4.742826e-04\n",
      "8887  8888        youth   0.005203  1.574781e+00   0.659291  1.157561e-03\n",
      "8888  8889      youzhni   0.007629  2.222285e+00   0.491417  8.627988e-04\n",
      "8889  8890          yu-   0.004758  1.354611e+00   0.418265  7.343599e-04\n",
      "8890  8891          yug   0.000041  7.117401e-08   0.000000  2.183305e-09\n",
      "8891  8892     yugoslav   0.004176  4.948838e+01  15.820595  2.777740e-02\n",
      "8892  8893   yugoslavia   0.011578  6.407368e+01  21.768024  3.821976e-02\n",
      "8893  8894  yugoslavian   0.011594  5.200910e+00   1.814555  3.185933e-03\n",
      "8894  8895         yuri   0.018397  2.049276e+00   0.507739  8.914598e-04\n",
      "8895  8896         yuxi   0.010819  1.354611e+00   0.418265  7.343599e-04\n",
      "8896  8897     zacharea   0.017215  2.485772e+00   0.507705  8.913961e-04\n",
      "8897  8898          zan   0.000147  3.119111e+00   0.540482  9.489453e-04\n",
      "8898  8899        zarko   0.006044  4.081127e+00   1.262433  2.216529e-03\n",
      "8899  8900      zawahri   0.002351  7.944005e-01   0.411467  7.224236e-04\n",
      "8900  8901         zeal   0.000789  2.642397e-01   0.136912  2.403682e-04\n",
      "8901  8902        zedek   0.007111  7.117401e-08   0.000000  2.183305e-09\n",
      "8902  8903         zein   0.006555  1.060325e+00   0.549112  9.640973e-04\n",
      "8903  8904         zell   0.003836  3.336962e+00   0.738692  1.296958e-03\n",
      "8904  8905        zemin   0.006830  5.883284e+00   1.228378  2.156738e-03\n",
      "8905  8906         zero   0.019958  8.665818e-01   0.355899  6.248644e-04\n",
      "8906  8907        zinni   0.023997  4.843446e+00   2.502540  4.393877e-03\n",
      "8907  8908        zivko   0.002301  6.765889e-01   0.208819  3.666214e-04\n",
      "8908  8909       zivkov   0.006340  1.354611e+00   0.418265  7.343599e-04\n",
      "8909  8910        zogbi   0.009333  3.336962e+00   0.738692  1.296958e-03\n",
      "8910  8911         zone   0.005806  7.458516e+00   2.180161  3.827861e-03\n",
      "8911  8912        zoran   0.005915  1.310480e+01   4.080964  7.165236e-03\n",
      "8912  8913        zuric   0.005658  6.765889e-01   0.208819  3.666214e-04\n",
      "\n",
      "[8913 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/.local/lib/python3.6/site-packages/ipykernel/__main__.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "cols = list(dict_df3)\n",
    "cols[4], cols[3], cols[5], cols[1], cols[0], cols[2] = cols[0], cols[1] , cols[2] , cols[3], cols[4], cols[5]\n",
    "dict_df3 = dict_df3.ix[:,cols]\n",
    "print(dict_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df3.columns = ['id','term','avg_tfidf','score_chi','score_llr','score_emi']\n",
    "# dict_df3.to_csv('output/feature_selection_df_rev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select top 500\n",
    "* 取各col的mean+1.45*std\n",
    "* 再去做投票，超過兩票的流下來看剩下哪幾個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_tfidf 0.024888109121963365\n"
     ]
    }
   ],
   "source": [
    "dict_df3 = pd.read_csv('output/feature_selection_df_rev.csv',index_col=None)\n",
    "threshold_tfidf = np.mean(dict_df3['avg_tfidf'])+2.5*np.std(dict_df3['avg_tfidf']) #1.45=>502 數字大嚴格\n",
    "threshold_chi = np.mean(dict_df3['score_chi'])+2.5*np.std(dict_df3['score_chi']) #1=>350\n",
    "threshold_llr = np.mean(dict_df3['score_llr'])+2.5*np.std(dict_df3['score_llr']) #1.75=>543\n",
    "threshold_emi = np.mean(dict_df3['score_emi'])+2.5*np.std(dict_df3['score_emi']) #1.75=>543\n",
    "\n",
    "print('avg_tfidf',threshold_tfidf)\n",
    "# dict_df3[dict_df3.score_llr>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0    id         term  avg_tfidf     score_chi  score_llr  \\\n",
      "0              0     1      abandon   0.005374  3.846051e+00   1.659552   \n",
      "1              1     2          abc   0.015857  1.555876e+00   0.899990   \n",
      "2              2     3  abcnews.com   0.033696  6.544514e-01   0.204845   \n",
      "3              3     4     abdallah   0.006597  5.290393e-01   0.274067   \n",
      "4              4     5        abdel   0.003924  7.944005e-01   0.411467   \n",
      "5              5     6      abdomin   0.010949  5.290393e-01   0.274067   \n",
      "6              6     7       abduct   0.016260  3.668540e-01   0.160386   \n",
      "7              7     8        abdul   0.000851  9.267195e-01   0.464110   \n",
      "8              8     9  abdul-karim   0.005135  2.937724e+00   1.519594   \n",
      "9              9    10     abdullah   0.006808  1.045027e+01   5.383307   \n",
      "10            10    11       abhorr   0.008349  4.723560e+00   0.599975   \n",
      "11            11    12         abid   0.000591  8.999763e-01   0.378774   \n",
      "12            12    13      abidjan   0.017659  5.488540e+01  10.299234   \n",
      "13            13    14         abil   0.001223  2.298259e+00   0.976184   \n",
      "14            14    15          abl   0.000976  1.925329e+00   0.970499   \n",
      "15            15    16        ablaz   0.006857  1.039403e+00   0.293008   \n",
      "16            16    17       aboard   0.006911  1.392154e+01   6.593954   \n",
      "17            17    18        abobo   0.005802  4.683628e+00   0.812945   \n",
      "18            18    19      abolish   0.006744  1.702640e+00   0.577250   \n",
      "19            19    20        abort   0.012137  2.420350e+00   0.677678   \n",
      "20            20    21         abou   0.000133  1.557905e+00   0.269511   \n",
      "21            21    22       abound   0.007865  7.431483e-01   0.360321   \n",
      "22            22    23      abraham   0.007256  6.706604e+00   1.472125   \n",
      "23            23    24       abroad   0.006570  7.390832e+00   2.623727   \n",
      "24            24    25        abrog   0.003322  7.117401e-08   0.000000   \n",
      "25            25    26       abrupt   0.001282  1.181549e+00   0.249705   \n",
      "26            26    27     abruptli   0.026259  1.241571e+00   0.253273   \n",
      "27            27    28       absenc   0.013634  2.259850e+00   0.600643   \n",
      "28            28    29       absent   0.001582  1.007732e+00   0.361637   \n",
      "29            29    30      absente   0.005075  5.783335e+00   1.352649   \n",
      "...          ...   ...          ...        ...           ...        ...   \n",
      "8883        8883  8884        young   0.004997  8.000081e+00   2.752543   \n",
      "8884        8884  8885      younger   0.004907  4.640895e+00   1.514666   \n",
      "8885        8885  8886     youngest   0.006362  1.384703e+00   0.521708   \n",
      "8886        8886  8887    youngster   0.005772  8.087003e-01   0.270136   \n",
      "8887        8887  8888        youth   0.005203  1.574781e+00   0.659291   \n",
      "8888        8888  8889      youzhni   0.007629  2.222285e+00   0.491417   \n",
      "8889        8889  8890          yu-   0.004758  1.354611e+00   0.418265   \n",
      "8890        8890  8891          yug   0.000041  7.117401e-08   0.000000   \n",
      "8891        8891  8892     yugoslav   0.004176  4.948838e+01  15.820595   \n",
      "8892        8892  8893   yugoslavia   0.011578  6.407368e+01  21.768024   \n",
      "8893        8893  8894  yugoslavian   0.011594  5.200910e+00   1.814555   \n",
      "8894        8894  8895         yuri   0.018397  2.049276e+00   0.507739   \n",
      "8895        8895  8896         yuxi   0.010819  1.354611e+00   0.418265   \n",
      "8896        8896  8897     zacharea   0.017215  2.485772e+00   0.507705   \n",
      "8897        8897  8898          zan   0.000147  3.119111e+00   0.540482   \n",
      "8898        8898  8899        zarko   0.006044  4.081127e+00   1.262433   \n",
      "8899        8899  8900      zawahri   0.002351  7.944005e-01   0.411467   \n",
      "8900        8900  8901         zeal   0.000789  2.642397e-01   0.136912   \n",
      "8901        8901  8902        zedek   0.007111  7.117401e-08   0.000000   \n",
      "8902        8902  8903         zein   0.006555  1.060325e+00   0.549112   \n",
      "8903        8903  8904         zell   0.003836  3.336962e+00   0.738692   \n",
      "8904        8904  8905        zemin   0.006830  5.883284e+00   1.228378   \n",
      "8905        8905  8906         zero   0.019958  8.665818e-01   0.355899   \n",
      "8906        8906  8907        zinni   0.023997  4.843446e+00   2.502540   \n",
      "8907        8907  8908        zivko   0.002301  6.765889e-01   0.208819   \n",
      "8908        8908  8909       zivkov   0.006340  1.354611e+00   0.418265   \n",
      "8909        8909  8910        zogbi   0.009333  3.336962e+00   0.738692   \n",
      "8910        8910  8911         zone   0.005806  7.458516e+00   2.180161   \n",
      "8911        8911  8912        zoran   0.005915  1.310480e+01   4.080964   \n",
      "8912        8912  8913        zuric   0.005658  6.765889e-01   0.208819   \n",
      "\n",
      "         score_emi  vote  \n",
      "0     2.913791e-03     0  \n",
      "1     1.580176e-03     0  \n",
      "2     3.596454e-04     0  \n",
      "3     4.811811e-04     0  \n",
      "4     7.224236e-04     0  \n",
      "5     4.811811e-04     0  \n",
      "6     2.815866e-04     0  \n",
      "7     8.148566e-04     0  \n",
      "8     2.668045e-03     0  \n",
      "9     9.451857e-03     0  \n",
      "10    1.053402e-03     0  \n",
      "11    6.650296e-04     0  \n",
      "12    1.808312e-02     0  \n",
      "13    1.713953e-03     0  \n",
      "14    1.703978e-03     0  \n",
      "15    5.144410e-04     0  \n",
      "16    1.157749e-02     0  \n",
      "17    1.427329e-03     0  \n",
      "18    1.013504e-03     0  \n",
      "19    1.189835e-03     0  \n",
      "20    4.731823e-04     0  \n",
      "21    6.326256e-04     0  \n",
      "22    2.584704e-03     0  \n",
      "23    4.606668e-03     0  \n",
      "24    2.183305e-09     0  \n",
      "25    4.384079e-04     0  \n",
      "26    4.446725e-04     0  \n",
      "27    1.054579e-03     0  \n",
      "28    6.349385e-04     0  \n",
      "29    2.374932e-03     0  \n",
      "...            ...   ...  \n",
      "8883  4.832844e-03     0  \n",
      "8884  2.659406e-03     0  \n",
      "8885  9.159885e-04     0  \n",
      "8886  4.742826e-04     0  \n",
      "8887  1.157561e-03     0  \n",
      "8888  8.627988e-04     0  \n",
      "8889  7.343599e-04     0  \n",
      "8890  2.183305e-09     0  \n",
      "8891  2.777740e-02     0  \n",
      "8892  3.821976e-02     0  \n",
      "8893  3.185933e-03     0  \n",
      "8894  8.914598e-04     0  \n",
      "8895  7.343599e-04     0  \n",
      "8896  8.913961e-04     0  \n",
      "8897  9.489453e-04     0  \n",
      "8898  2.216529e-03     0  \n",
      "8899  7.224236e-04     0  \n",
      "8900  2.403682e-04     0  \n",
      "8901  2.183305e-09     0  \n",
      "8902  9.640973e-04     0  \n",
      "8903  1.296958e-03     0  \n",
      "8904  2.156738e-03     0  \n",
      "8905  6.248644e-04     0  \n",
      "8906  4.393877e-03     0  \n",
      "8907  3.666214e-04     0  \n",
      "8908  7.343599e-04     0  \n",
      "8909  1.296958e-03     0  \n",
      "8910  3.827861e-03     0  \n",
      "8911  7.165236e-03     0  \n",
      "8912  3.666214e-04     0  \n",
      "\n",
      "[8913 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = dict_df3[dict_df3['avg_tfidf']>threshold_tfidf]\n",
    "df2 = dict_df3[dict_df3['score_chi']>threshold_chi]\n",
    "df3 = dict_df3[dict_df3['score_llr']>threshold_llr]\n",
    "df4 = dict_df3[dict_df3['score_emi']>threshold_emi]\n",
    "df_vote = dict_df3\n",
    "df_vote['vote']=0\n",
    "print(df_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vote.loc[df1.id-1,'vote'] += 1\n",
    "df_vote.loc[df2.id-1,'vote'] += 1\n",
    "df_vote.loc[df3.id-1,'vote'] += 1\n",
    "df_vote.loc[df4.id-1,'vote'] += 1\n",
    "# df_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id         term  vote\n",
      "12      13      abidjan     3\n",
      "81      82         aden     3\n",
      "130    131      african     3\n",
      "134    135   aftershock     3\n",
      "136    137       agassi     4\n",
      "202    203      alassan     3\n",
      "208    209      alberto     3\n",
      "310    311         andr     3\n",
      "385    386      appoint     3\n",
      "396    397        april     3\n",
      "473    474     ashcroft     3\n",
      "520    521       attack     3\n",
      "545    546   australian     3\n",
      "625    626       ballot     3\n",
      "651    652       barent     3\n",
      "694    695         beat     3\n",
      "721    722      belgrad     3\n",
      "763    764         bhuj     3\n",
      "857    858         boat     3\n",
      "860    861         bodi     3\n",
      "871    872         bomb     3\n",
      "881    882         bone     3\n",
      "955    956        bribe     3\n",
      "1040  1041         buri     3\n",
      "1057  1058  businessman     3\n",
      "1101  1102     campaign     3\n",
      "1108  1109       cancer     3\n",
      "1109  1110       candid     3\n",
      "1131  1132     capriati     4\n",
      "1155  1156     carnahan     4\n",
      "...    ...          ...   ...\n",
      "7237  7238     slobodan     3\n",
      "7392  7393          spi     3\n",
      "7414  7415        sport     3\n",
      "7443  7444          st.     3\n",
      "7634  7635     submarin     4\n",
      "7665  7666       suicid     3\n",
      "7719  7720     survivor     3\n",
      "7883  7884        tenni     3\n",
      "7903  7904    terrorist     3\n",
      "7912  7913         texa     3\n",
      "8024  8025         titl     3\n",
      "8045  8046         toll     3\n",
      "8072  8073      torpedo     3\n",
      "8086  8087   tournament     3\n",
      "8447  8448          uss     3\n",
      "8468  8469     valentin     3\n",
      "8505  8506         venu     3\n",
      "8536  8537      vietnam     4\n",
      "8537  8538    vietnames     3\n",
      "8582  8583     vladimir     3\n",
      "8583  8584    vladimiro     3\n",
      "8591  8592     vojislav     3\n",
      "8600  8601         vote     3\n",
      "8751  8752        widow     3\n",
      "8763  8764          win     3\n",
      "8767  8768       winner     3\n",
      "8871  8872        yemen     3\n",
      "8872  8873       yemeni     3\n",
      "8891  8892     yugoslav     3\n",
      "8892  8893   yugoslavia     3\n",
      "\n",
      "[186 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_vote_ = df_vote[df_vote.vote>2] #(1,2)=>375 #(1,1)=>422 #(1.6,2)=>482 #(2,2)=>330 #(1,3)=>100\n",
    "df_vote_ = df_vote_.filter(['id','term','vote'])\n",
    "print(df_vote_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vote_.to_csv('output/500terms_df_rev5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "* 7-fold\n",
    "* MNB\n",
    "* BNB\n",
    "* self-train\n",
    "* ens voting (BNB lower weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "df_vote = pd.read_csv('output/500terms_df_rev5.csv',index_col=False)\n",
    "terms_li = list(set(df_vote.term.tolist()))\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "print(len(terms_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/training.txt','r') as f:\n",
    "    train_id = f.read().splitlines()\n",
    "train_dict = {}\n",
    "\n",
    "for trainid in train_id:\n",
    "    trainid = trainid.split(' ')\n",
    "    trainid = list(filter(None, trainid))\n",
    "    train_dict[trainid[0]] = trainid[1:]\n",
    "# train_dict #class:doc_id\n",
    "train_dict = pickle.load(open('data/train_dict.pkl','rb'))\n",
    "in_dir = 'data/IRTM/'\n",
    "train_dict_ = {}\n",
    "class_token = []\n",
    "class_token_dict = {}\n",
    "train_X = []\n",
    "train_Y= []\n",
    "train_ids = []\n",
    "for c,d in tqdm(train_dict.items()):\n",
    "    for doc in d:\n",
    "        train_ids.append(doc)\n",
    "        trainX = np.array([0]*len(terms_li))\n",
    "        f = open('data/IRTM/'+doc+'.txt')\n",
    "        texts = f.read()\n",
    "        f.close()\n",
    "        tokens_all = preprocess(texts)\n",
    "        tokens_all = tokens_all.split(' ')\n",
    "#         tokens_all = list(filter(None,tokens_all))\n",
    "        tokens_all = dict(Counter(tokens_all))\n",
    "        for key,value in tokens_all.items():\n",
    "            if key in terms_li:\n",
    "                trainX[terms_li.index(key)] = int(value)\n",
    "#         trainX = np.array(trainX)\n",
    "        \n",
    "#         for token in tokens_all:\n",
    "#             if token in terms_li:\n",
    "#                 ind = terms_li.index(token)\n",
    "#                 trainX[ind]+=1\n",
    "        train_X.append(trainX)\n",
    "        train_Y.append(int(c))\n",
    "        \n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n",
    "\n",
    "#         tokens_all = list(set(filter(None,tokens_all)))\n",
    "#         class_token.append(tokens_all)\n",
    "#     class_token_dict[c]=class_token\n",
    "#     class_token=[]\n",
    "# len(class_token_dict['1'])\n",
    "# print(train_X.shape , train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:07<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#建立term index matrix\n",
    "tokens_all_class=[]\n",
    "term_tf_mat=[]\n",
    "for c,d in tqdm(train_dict.items()):\n",
    "    for doc in d:\n",
    "        f = open('data/IRTM/'+doc+'.txt')\n",
    "        texts = f.read()\n",
    "        f.close()\n",
    "        tokens_all = preprocess(texts)\n",
    "        tokens_all = tokens_all.split(' ')\n",
    "        tokens_all = list(filter(None,tokens_all))\n",
    "        tokens_all_class.extend(tokens_all)\n",
    "    tokens_all = dict(Counter(tokens_all_class))\n",
    "    term_tf_mat.append(tokens_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MNB(train_set=train_dict,term_list=terms_li,term_tf_mat=term_tf_mat):\n",
    "    prior = np.zeros(len(train_set))\n",
    "    cond_prob = np.zeros((len(train_set), len(term_list)))\n",
    "    \n",
    "    for i,docs in train_set.items(): #13 classes 1~13\n",
    "        prior[int(i)-1] = len(docs)/len(train_ids) #那個類別的文章有幾個/總共的文章數目 0~12\n",
    "        token_count=0\n",
    "        class_tf = np.zeros(len(term_list))\n",
    "        for idx,term in enumerate(term_list):\n",
    "            try:\n",
    "                class_tf[idx] = term_tf_mat[int(i)-1][term]  #term在class的出現次數\n",
    "            except:\n",
    "                token_count+=1\n",
    "\n",
    "        class_tf = class_tf + np.ones(len(term_list)) #smoothing (可改)\n",
    "        class_tf = class_tf/(sum(class_tf) +token_count) #該class總共的token數(可改)\n",
    "        cond_prob[int(i)-1] = class_tf #0~12\n",
    "    return prior, cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05496829 0.1141649  0.03488372 0.03276956 0.06976744 0.06553911\n",
      " 0.06236786 0.05813953 0.27378436 0.04968288 0.06976744 0.06553911\n",
      " 0.04862579] [[0.00048852 0.00048852 0.00048852 ... 0.00048852 0.00097704 0.00048852]\n",
      " [0.0376849  0.00105659 0.0001174  ... 0.0002348  0.01995774 0.00305236]\n",
      " [0.03390726 0.00095067 0.00010563 ... 0.00021126 0.01795711 0.00274638]\n",
      " ...\n",
      " [0.00917063 0.00505321 0.00232608 ... 0.00310144 0.01069462 0.00689803]\n",
      " [0.0083773  0.00461606 0.00212485 ... 0.00285756 0.01079523 0.00642341]\n",
      " [0.00802339 0.00444444 0.00203509 ... 0.00273684 0.01083041 0.00633918]]\n"
     ]
    }
   ],
   "source": [
    "prior,cond_prob = train_MNB()\n",
    "print(prior,cond_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_MNB(test_id,prob=False,prior=prior,cond_prob=cond_prob,term_list=terms_li):\n",
    "    f = open('data/IRTM/'+str(test_id)+'.txt')\n",
    "    texts = f.read()\n",
    "    f.close()\n",
    "    tokens_all = preprocess(texts)\n",
    "    tokens_all = tokens_all.split(' ')\n",
    "    tokens_all = list(filter(None,tokens_all))\n",
    "    \n",
    "    class_scores = []\n",
    "#     score = 0\n",
    "    for i in range(13):\n",
    "        score=0\n",
    "#         print(prior[i])\n",
    "        score += math.log(prior[i],10)\n",
    "        for token in tokens_all:\n",
    "            if token in term_list:\n",
    "                score += math.log(cond_prob[i][term_list.index(token)])\n",
    "        class_scores.append(score)\n",
    "    if prob:\n",
    "        return np.array(class_scores)\n",
    "    else:\n",
    "        return(np.argmax(class_scores)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for testing class function only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB(input_X,input_Y=None,prior_log_class=None,log_prob_feature=None,train=True,prob=False,smooth=1.0):\n",
    "    if train:\n",
    "        sample_num = input_X.shape[0]\n",
    "        match_data = [[x for x, t in zip(input_X, input_Y) if t == c] for c in np.unique(input_Y)]\n",
    "        prior_log_class = [np.log(len(i) / sample_num) for i in match_data]\n",
    "        counts = np.array([np.array(i).sum(axis=0) for i in match_data]) + smooth\n",
    "        log_prob_feature = np.log(counts / counts.sum(axis=1)[np.newaxis].T)\n",
    "        return prior_log_class,log_prob_feature\n",
    "    else:\n",
    "        probability = [(log_prob_feature * x).sum(axis=1) + prior_log_class for x in input_X]\n",
    "        if prob:\n",
    "            return probability\n",
    "        else:\n",
    "            ans = np.argmax(probability,axis=1)\n",
    "            return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB(object):\n",
    "    def __init__(self, alpha=1.0, binarize=0.0):\n",
    "        self.alpha = alpha\n",
    "        self.binarize = binarize\n",
    "    def _binarize_X(self, X):\n",
    "        return np.where(X > self.binarize, 1, 0) if self.binarize != None else X\n",
    "    def fit(self, X, y):\n",
    "        X = self._binarize_X(X)\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [np.log(len(i) / count_sample) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        smoothing = 2 * self.alpha\n",
    "        n_doc = np.array([len(i) + smoothing for i in separated])\n",
    "        self.feature_prob_ = count / n_doc[np.newaxis].T\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        X = self._binarize_X(X)\n",
    "        return [(np.log(self.feature_prob_) * x + \\\n",
    "                 np.log(1 - self.feature_prob_) * np.abs(x - 1)\n",
    "                ).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self._binarize_X(X)\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "df_vote = pd.read_csv('output/500terms_df_rev5.csv',index_col=False)\n",
    "terms_li = list(set(df_vote.term.tolist()))\n",
    "print(len(terms_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:11<00:00, 75.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 2, 2, 4, 2, 2, 13, 9, 9, 9, 9, 11, 9, 9, 9, 11, 9, 9, 11, 9, 9, 9, 9, 10, 9, 9, 9, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 11, 9, 9, 11, 9, 9, 9, 9, 9, 9, 9, 13, 9, 13, 9, 9, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 13, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 9, 9, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 1, 11, 9, 9, 11, 9, 9, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 9, 9, 12, 9, 9, 11, 11, 9, 9, 10, 11, 11, 11, 9, 9, 11, 11, 9, 11, 9, 11, 9, 1, 11, 9, 11, 9, 11, 11, 9, 10, 9, 9, 9, 1, 10, 10, 1, 9, 9, 11, 10, 1, 12, 9, 10, 1, 10, 1, 1, 9, 1, 9, 1, 1, 9, 10, 10, 10, 12, 10, 1, 12, 9, 1, 1, 11, 11, 1, 11, 9, 10, 9, 9, 9, 9, 11, 9, 9, 11, 9, 9, 1, 13, 10, 1, 1, 10, 1, 1, 9, 13, 1, 13, 1, 9, 10, 9, 1, 10, 9, 11, 10, 10, 10, 11, 1, 9, 9, 9, 9, 1, 9, 9, 11, 9, 9, 9, 9, 9, 1, 9, 9, 9, 1, 9, 1, 1, 9, 9, 11, 11, 13, 9, 9, 1, 1, 9, 1, 11, 1, 11, 9, 11, 11, 9, 1, 11, 11, 13, 11, 1, 11, 13, 11, 11, 11, 11, 11, 11, 11, 11, 12, 11, 13, 13, 11, 9, 9, 11, 12, 11, 11, 11, 9, 11, 9, 9, 1, 9, 10, 1, 13, 13, 13, 13, 13, 9, 1, 9, 9, 13, 13, 13, 13, 13, 13, 13, 9, 9, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 13, 12, 13, 13, 13, 13, 13, 8, 12, 12, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 9, 9, 12, 9, 12, 12, 9, 8, 1, 12, 13, 9, 12, 9, 2, 2, 12, 9, 12, 12, 8, 12, 12, 13, 8, 12, 12, 9, 9, 9, 2, 9, 8, 11, 8, 8, 9, 9, 8, 8, 8, 8, 8, 8, 8, 2, 8, 9, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 9, 12, 2, 2, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 2, 8, 10, 11, 9, 9, 13, 12, 11, 13, 9, 2, 8, 12, 12, 13, 12, 12, 9, 12, 1, 9, 9, 12, 12, 9, 11, 9, 9, 9, 9, 7, 9, 7, 7, 9, 9, 7, 7, 9, 10, 7, 9, 9, 9, 9, 9, 7, 9, 9, 7, 9, 7, 9, 9, 9, 9, 7, 12, 8, 7, 12, 7, 7, 7, 7, 7, 7, 7, 7, 9, 11, 1, 1, 4, 11, 12, 6, 6, 4, 6, 3, 6, 6, 5, 5, 6, 5, 5, 3, 6, 13, 5, 12, 7, 7, 6, 3, 6, 12, 5, 3, 13, 6, 13, 6, 6, 6, 7, 6, 8, 6, 6, 6, 9, 3, 5, 3, 6, 5, 6, 6, 11, 7, 5, 5, 5, 5, 6, 5, 3, 9, 6, 5, 5, 5, 5, 5, 9, 13, 11, 9, 5, 9, 6, 9, 5, 8, 3, 5, 5, 5, 4, 4, 5, 9, 13, 4, 4, 4, 13, 4, 5, 4, 8, 13, 8, 13, 13, 8, 5, 5, 4, 5, 5, 8, 5, 8, 5, 4, 5, 5, 6, 6, 7, 4, 7, 5, 7, 4, 7, 7, 7, 5, 5, 5, 5, 7, 5, 7, 7, 7, 7, 7, 7, 4, 7, 5, 5, 5, 5, 7, 5, 11, 7, 5, 7, 5, 7, 7, 5, 7, 7, 4, 5, 4, 5, 3, 4, 7, 5, 13, 7, 4, 5, 5, 13, 5, 5, 5, 13, 5, 13, 5, 6, 5, 13, 6, 5, 6, 5, 6, 6, 5, 6, 6, 5, 6, 12, 6, 12, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 8, 5, 7, 6, 6, 6, 6, 6, 5, 6, 5, 5, 4, 6, 6, 4, 4, 6, 10, 6, 6, 13, 6, 6, 13, 13, 6, 12, 6, 6, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/training.txt','r') as f:\n",
    "    train_id = f.read().splitlines()\n",
    "train_dict = {}\n",
    "test_id = []\n",
    "train_ids=[]\n",
    "for trainid in train_id:\n",
    "    trainid = trainid.split(' ')\n",
    "    trainid = list(filter(None, trainid))\n",
    "    train_ids.extend(trainid[1:])\n",
    "for i in range(1095):\n",
    "    if str(i+1) not in train_ids:\n",
    "        test_id.append(i+1)\n",
    "ans=[]\n",
    "for doc in tqdm(test_id):\n",
    "    ans.append(predict_MNB(doc))\n",
    "print(ans)\n",
    "df_ans = pd.DataFrame(list(zip(test_id,ans)),columns=['id','Value'])\n",
    "# df_ans.to_csv('output/MNB.csv',index=False)\n",
    "# df_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine all prediction df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "in_dir = './data/'\n",
    "prefixed = [filename for filename in os.listdir(in_dir) if filename.startswith(\"MNB\")]\n",
    "df_from_each_file = [pd.read_csv(in_dir+f) for f in prefixed]\n",
    "# prefixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id   0   1   2   3   4   5   6   7   8\n",
      "0      17   2   2   2   2   2   2   2   2   2\n",
      "1      18   2   2   2   2   2   2   2   2   2\n",
      "2      20   2   2   2   2   2   2   2   2   2\n",
      "3      21   2   2   2   2   2   2   2   2   2\n",
      "4      22   2   2   2   2   2   2   9   2   2\n",
      "5      23   2   2   2   2   2   2   2   2   2\n",
      "6      24   2   2   2   2   2   2   2   2   2\n",
      "7      25   2   2   2   2   2   2   2   2   2\n",
      "8      26   9   2   2   2   2   2   9   2   2\n",
      "9      27   2   2   2   2   2   2   2   2   2\n",
      "10     28   2   2   2   2   2   2   9   2   2\n",
      "11     30   2   2   2   2   2   2   2   2   2\n",
      "12     32   2   2   2   2   2   2   9   2   2\n",
      "13     33   2   2   2   2   2   2   2   2   2\n",
      "14     34   2   2   2   2   2   2   2   2   2\n",
      "15     35   2   2   2   2   2   2   2   2   2\n",
      "16     36   2   2   2   2   2   2   9   2   2\n",
      "17     37   2   2   2   2   2   2   9   2   2\n",
      "18     38   2   2   2   2   2   2   2   2   2\n",
      "19     39   2   2   2   2   2   2   2   2   2\n",
      "20     40   2   2   2   2   2   2   2   2   2\n",
      "21     41   2   2   2   2   2   2   9   2   2\n",
      "22     42  11   2   2   2   2   2   2   2   2\n",
      "23     43   2   2   2   2   2   2   2   2   2\n",
      "24     45   2   2   2   2   2   2   2   2   2\n",
      "25     46   2   2   2   2   2   2   9   2   2\n",
      "26     47   2   2   2   2   2   2   2   2   2\n",
      "27     48   2   2   2   2   2   2   2   2   2\n",
      "28     49   2   2   2   2   2   2   2   2   2\n",
      "29     50   2   2   2   2   2   2   2   2   2\n",
      "..    ...  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
      "870  1066   6   6   6   6   6   6   6   6   6\n",
      "871  1067   9   9   9   8   9   9   9   9   8\n",
      "872  1068   6   6   6   6   6   6   6   6   6\n",
      "873  1069   6   6   6   6   6   6   9   6   6\n",
      "874  1070   4   4   4   4   4   4   9   4   4\n",
      "875  1071   6   6   6   6   6   6   6   6   6\n",
      "876  1072   6   6   6   6   6   6   6   6   6\n",
      "877  1073   4   4   4   4   4   4   9   4   4\n",
      "878  1074   4   4   4   4   4   4   9   4  13\n",
      "879  1075   6   6   6   6   6   6   6   6   6\n",
      "880  1076  12  12  12  12  12  12  12  12  12\n",
      "881  1077   6   6   6   6   6   6   6   6   6\n",
      "882  1078   6   6   6   6   6   6   6   6   6\n",
      "883  1079   9   9   9   9   9   9   9   9   9\n",
      "884  1080   6   6   6   6   6   6   6   6   6\n",
      "885  1081   6   6   6   6   6   6   6   6   6\n",
      "886  1082   6   6   6   6   6   6   6   6   6\n",
      "887  1083   6   6   6   6   6   6   6   6   6\n",
      "888  1084   6   6   6   6   6   6   6   6   6\n",
      "889  1085   6   6   6   6   6   6   6   6   6\n",
      "890  1086   6   6   6   6   6   6   6   6   6\n",
      "891  1087   6   6   6   6   6   6   6   6   6\n",
      "892  1088   6   6   6   6   6   6   6   6   6\n",
      "893  1089   6   6   6   6   6   6   6   6   6\n",
      "894  1090   6   6   6   6   6   6   6   6   6\n",
      "895  1091   6   6   6   6   6   6   6   6   6\n",
      "896  1092   6   6   6   6   6   6   6   6   6\n",
      "897  1093   6   6   6   6   6   6   6   6   6\n",
      "898  1094   6   6   6   6   6   6   6   6   6\n",
      "899  1095   9   9   9   9   9   9   9   9   9\n",
      "\n",
      "[900 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df = functools.reduce(lambda left,right: pd.merge(left,right,on='id'), df_from_each_file)\n",
    "merged_df.columns = ['id',0,1,2,3,4,5,6,7,8]\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  Value\n",
      "0      17      2\n",
      "1      18      2\n",
      "2      20      2\n",
      "3      21      2\n",
      "4      22      2\n",
      "5      23      2\n",
      "6      24      2\n",
      "7      25      2\n",
      "8      26      2\n",
      "9      27      2\n",
      "10     28      2\n",
      "11     30      2\n",
      "12     32      2\n",
      "13     33      2\n",
      "14     34      2\n",
      "15     35      2\n",
      "16     36      2\n",
      "17     37      2\n",
      "18     38      2\n",
      "19     39      2\n",
      "20     40      2\n",
      "21     41      2\n",
      "22     42      2\n",
      "23     43      2\n",
      "24     45      2\n",
      "25     46      2\n",
      "26     47      2\n",
      "27     48      2\n",
      "28     49      2\n",
      "29     50      2\n",
      "..    ...    ...\n",
      "870  1066      6\n",
      "871  1067      9\n",
      "872  1068      6\n",
      "873  1069      6\n",
      "874  1070      4\n",
      "875  1071      6\n",
      "876  1072      6\n",
      "877  1073      4\n",
      "878  1074      4\n",
      "879  1075      6\n",
      "880  1076     12\n",
      "881  1077      6\n",
      "882  1078      6\n",
      "883  1079      9\n",
      "884  1080      6\n",
      "885  1081      6\n",
      "886  1082      6\n",
      "887  1083      6\n",
      "888  1084      6\n",
      "889  1085      6\n",
      "890  1086      6\n",
      "891  1087      6\n",
      "892  1088      6\n",
      "893  1089      6\n",
      "894  1090      6\n",
      "895  1091      6\n",
      "896  1092      6\n",
      "897  1093      6\n",
      "898  1094      6\n",
      "899  1095      9\n",
      "\n",
      "[900 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df01 = pd.DataFrame(merged_df.mode(axis=1)[0])\n",
    "df02 = pd.DataFrame(merged_df['id'])\n",
    "df_ans = pd.concat([df02,df01],axis=1)\n",
    "df_ans = df_ans.astype('int')\n",
    "df_ans.columns = ['id','Value']\n",
    "df_ans.to_csv('output/voting_rev.csv',index=False)\n",
    "print(df_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do only in first time only ro produce new train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df1 = merged_df[(merged_df[0] == merged_df[1])&(merged_df[2]==merged_df[3])&(merged_df[1]==merged_df[2])]\n",
    "# df1.reset_index(inplace=True,drop=True)\n",
    "# df1['class'] = df1[0]\n",
    "# df1 = df1.filter(['id','class'])\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df1[df1['class']=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/training.txt','r') as f:\n",
    "#     train_id = f.read().splitlines()\n",
    "# train_dict = {}\n",
    "# for trainid in train_id:\n",
    "#     trainid = trainid.split(' ')\n",
    "#     trainid = list(filter(None, trainid))\n",
    "#     train_dict[trainid[0]] = trainid[1:]\n",
    "# train_dict #class:doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(13):\n",
    "#     df1 = df1.astype(str)\n",
    "#     li = df1[df1['class'] == str(i+1)]['id'].tolist()\n",
    "#     train_dict[str(i+1)].extend(li)\n",
    "# train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(obj=train_dict,file=open('data/train_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:06<00:00, 131.85it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/training.txt','r') as f:\n",
    "    train_id = f.read().splitlines()\n",
    "train_dict = {}\n",
    "test_id = []\n",
    "train_ids=[]\n",
    "for trainid in train_id:\n",
    "    trainid = trainid.split(' ')\n",
    "    trainid = list(filter(None, trainid))\n",
    "    train_ids.extend(trainid[1:])\n",
    "for i in range(1095):\n",
    "    if str(i+1) not in train_ids:\n",
    "        test_id.append(i+1)\n",
    "#     train_dict[trainid[0]] = trainid[1:]\n",
    "# train_dict #class:doc_id\n",
    "in_dir = 'data/IRTM/'\n",
    "train_dict_ = {}\n",
    "class_token = []\n",
    "class_token_dict = {}\n",
    "test_X = []\n",
    "# train_Y= []\n",
    "# for c,d in tqdm(train_dict.items()):\n",
    "for doc in tqdm(test_id):\n",
    "    testX = np.array([0]*len(terms_li))\n",
    "    f = open('data/IRTM/'+str(doc)+'.txt')\n",
    "    texts = f.read()\n",
    "    f.close()\n",
    "    tokens_all = preprocess(texts)\n",
    "    tokens_all = tokens_all.split(' ')\n",
    "#         tokens_all = list(filter(None,tokens_all))\n",
    "    tokens_all = dict(Counter(tokens_all))\n",
    "    for key,value in tokens_all.items():\n",
    "        if key in terms_li:\n",
    "            testX[terms_li.index(key)] = int(value)\n",
    "\n",
    "    test_X.append(testX)\n",
    "        \n",
    "test_X = np.array(test_X)\n",
    "# print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_X.shape, train_Y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_ans = pd.DataFrame(list(zip(test_id,ans2)),columns=['id','Value'])\n",
    "# df_ans.to_csv('output/MNB02.csv',index=False)\n",
    "# df_ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
